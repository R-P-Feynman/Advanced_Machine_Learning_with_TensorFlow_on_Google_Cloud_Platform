{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.logging.set_verbosity(v = tf.logging.INFO)\n",
    "\n",
    "HEIGHT = 28\n",
    "WIDTH = 28\n",
    "NCLASSES = 10\n",
    "\n",
    "def linear_model(img, mode, hparams):\n",
    "    X = tf.reshape(tensor = img, shape = [-1,HEIGHT * WIDTH]) #flatten\n",
    "    ylogits = tf.layers.dense(input = X, units = NCLASSES, activation = None)\n",
    "    return ylogits, NCLASSES\n",
    "\n",
    "def dnn_model(img, mode, hparams):\n",
    "    X = tf.reshape(tensor = img, shape = [-1, HEIGHT * WIDTH]) #flatten\n",
    "    h1 = tf.layers.dense(inputs = X, units = 300, activation = tf.nn.relu)\n",
    "    h2 = tf.layers.dense(inputs = h1, units = 100, activation = tf.nn.relu)\n",
    "    h3 = tf.layers.dense(inputs = h2, units = 30, activation = tf.nn.relu)\n",
    "    ylogits = tf.layers.dense(inputs = h3, units = NCLASSES, activation = None)\n",
    "    return ylogits, NCLASSES\n",
    "\n",
    "def dnn_dropout_model(img, mode, hparams):\n",
    "    dprob = hparams.get(\"dprob\", 0.1)\n",
    "\n",
    "    X = tf.reshape(tensor = img, shape = [-1, HEIGHT * WIDTH]) #flatten\n",
    "    h1 = tf.layers.dense(inputs = X, units = 300, activation = tf.nn.relu)\n",
    "    h2 = tf.layers.dense(inputs = h1, units = 100, activation = tf.nn.relu)\n",
    "    h3 = tf.layers.dense(inputs = h2, units = 30, activation = tf.nn.relu)\n",
    "    h3d = tf.layers.dropout(inputs = h3, rate = dprob, training = (mode == tf.estimator.ModeKeys.TRAIN)) #only dropout when training\n",
    "    ylogits = tf.layers.dense(inputs = h3d, units = NCLASSES, activation = None)\n",
    "    return ylogits, NCLASSES\n",
    "\n",
    "def cnn_model(img, mode, hparams):\n",
    "    ksize1 = hparams.get(\"ksize1\", 5)\n",
    "    ksize2 = hparams.get(\"ksize2\", 5)\n",
    "    nfil1 = hparams.get(\"nfil1\", 10)\n",
    "    nfil2 = hparams.get(\"nfil2\", 20)\n",
    "    dprob = hparams.get(\"dprob\", 0.25)\n",
    "\n",
    "    c1 = tf.layers.conv2d(inputs = img, filters = nfil1,\n",
    "                          kernel_size = ksize1, strides = 1,\n",
    "                          padding = \"same\", activation = tf.nn.relu) # shape = (batch_size, HEIGHT, WIDTH, nfil1)\n",
    "    \n",
    "    p1 = tf.layers.max_pooling2d(inputs = c1, pool_size = 2, strides = 2) # shape = (batch_size, HEIGHT // 2, WIDTH // 2, nfil1)\n",
    "    \n",
    "    c2 = tf.layers.conv2d(inputs = p1, filters = nfil2,\n",
    "                          kernel_size = ksize2, strides = 1, \n",
    "                          padding = \"same\", activation = tf.nn.relu) # shape = (batch_size, HEIGHT // 2, WIDTH // 2, nfil2)\n",
    "    \n",
    "    p2 = tf.layers.max_pooling2d(inputs = c2, pool_size = 2, strides = 2) # shape = (batch_size, HEIGHT // 4, WIDTH // 4, nfil2)\n",
    "\n",
    "    outlen = p2.shape[1] * p2.shape[2] * p2.shape[3] # HEIGHT // 4 * WIDTH // 4 * nfil2\n",
    "    p2flat = tf.reshape(tensor = p2, shape = [-1, outlen]) # shape = (batch_size, HEIGHT // 4 * WIDTH // 4 * nfil2)\n",
    "\n",
    "    # Apply batch normalization\n",
    "    if hparams[\"batch_norm\"]:\n",
    "        h3 = tf.layers.dense(inputs = p2flat, units = 300, activation = None)\n",
    "        h3 = tf.layers.batch_normalization(inputs = h3, training = (mode == tf.estimator.ModeKeys.TRAIN)) # only batchnorm when training\n",
    "        h3 = tf.nn.relu(features = h3)\n",
    "    else:  \n",
    "        h3 = tf.layers.dense(inputs = p2flat, units = 300, activation = tf.nn.relu)\n",
    "  \n",
    "    # Apply dropout\n",
    "    h3d = tf.layers.dropout(inputs = h3, rate = dprob, training = (mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "    ylogits = tf.layers.dense(inputs = h3d, units = NCLASSES, activation = None)\n",
    "  \n",
    "    # Apply batch normalization once more\n",
    "    if hparams[\"batch_norm\"]:\n",
    "        ylogits = tf.layers.batch_normalization(inputs = ylogits, training = (mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "    return ylogits, NCLASSES\n",
    "\n",
    "def serving_input_fn():\n",
    "    # Input will be rank 3\n",
    "    feature_placeholders = {\"image\": tf.placeholder(dtype = tf.float32, shape = [None, HEIGHT, WIDTH])}\n",
    "    # But model function requires rank 4\n",
    "    features = {\"image\": tf.expand_dims(input = feature_placeholders[\"image\"], axis = -1)} \n",
    "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)\n",
    "\n",
    "def image_classifier(features, labels, mode, params):\n",
    "    model_functions = {\n",
    "        \"linear\": linear_model,\n",
    "        \"dnn\": dnn_model,\n",
    "        \"dnn_dropout\": dnn_dropout_model,\n",
    "        \"cnn\": cnn_model}\n",
    "    \n",
    "    model_function = model_functions[params[\"model\"]]\n",
    "    \n",
    "    ylogits, nclasses = model_function(features[\"image\"], mode, params)\n",
    "\n",
    "    probabilities = tf.nn.softmax(logits = ylogits)\n",
    "    class_ids = tf.cast(x = tf.argmax(input = probabilities, axis = 1), dtype = tf.uint8)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss = tf.reduce_mean(input_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(logits = ylogits, labels = labels))\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # This is needed for batch normalization, but has no effect otherwise\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                train_op = tf.contrib.layers.optimize_loss(\n",
    "                    loss = loss, \n",
    "                    global_step = tf.train.get_global_step(),\n",
    "                    learning_rate = params[\"learning_rate\"], \n",
    "                    optimizer = \"Adam\")\n",
    "            eval_metric_ops = None\n",
    "        else:\n",
    "            train_op = None\n",
    "            eval_metric_ops =  {\"accuracy\": tf.metrics.accuracy(labels = tf.argmax(input = labels, axis = 1), predictions = class_ids)}\n",
    "    else:\n",
    "        loss = None\n",
    "        train_op = None\n",
    "        eval_metric_ops = None\n",
    " \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        predictions = {\"probabilities\": probabilities, \"class_ids\": class_ids},\n",
    "        loss = loss,\n",
    "        train_op = train_op,\n",
    "        eval_metric_ops = eval_metric_ops,\n",
    "        export_outputs = {\"predictions\":tf.estimator.export.PredictOutput({\"probabilities\": probabilities, \"class_ids\": class_ids})}\n",
    "    )\n",
    "\n",
    "def train_and_evaluate(output_dir, hparams):\n",
    "    tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "    \n",
    "    EVAL_INTERVAL = 60\n",
    "    \n",
    "    mnist = input_data.read_data_sets(\"mnist/data\", one_hot = True, reshape = False)\n",
    "    \n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x = {\"image\": mnist.train.images},\n",
    "        y = mnist.train.labels,\n",
    "        batch_size = 100,\n",
    "        num_epochs = None,\n",
    "        shuffle = True,\n",
    "        queue_capacity = 5000\n",
    "    )\n",
    "\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x = {\"image\": mnist.test.images},\n",
    "        y = mnist.test.labels,\n",
    "        batch_size = 100,\n",
    "        num_epochs = 1,\n",
    "        shuffle = False,\n",
    "        queue_capacity = 5000\n",
    "    )\n",
    "\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn = image_classifier,\n",
    "        model_dir = output_dir,\n",
    "        params = hparams)\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = train_input_fn,\n",
    "        max_steps = hparams[\"train_steps\"])\n",
    "\n",
    "    exporter = tf.estimator.LatestExporter(name = \"exporter\", serving_input_receiver_fn = serving_input_fn)\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = eval_input_fn,\n",
    "        steps = None,\n",
    "        exporters = exporter)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(estimator = estimator, train_spec = train_spec, eval_spec = eval_spec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
